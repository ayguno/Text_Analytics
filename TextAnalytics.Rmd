---
title: "Text Analytics with R"
output:
  html_document:
    depth: 6
    df_print: paged
    highlight: tango
    number_sections: yes
    theme: cerulean
    toc: yes
  word_document:
    toc: yes
    highlight: default
    df_print: kable
  pdf_document:
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(results = "markup", fig.align = "center",
                      fig.height= 8, fig.width= 8,message=FALSE,warning=FALSE)
```

# Introduction

Here is initial data processing and manipulation.

```{r}
############################################
# Text Analytics (learning from D.Langer)
# Mainly following playlist at : https://www.youtube.com/watch?v=4vuw0AsHeGw&list=PL8eNk_zTBST8olxIRFoo0YeXxEOkYdoxi
############################################

#install.packages(c("quanteda","irlba"))

spam.raw <- read.csv(file = 'spam.csv' , stringsAsFactors =  FALSE)
spam.raw <- spam.raw[,1:2]

names(spam.raw) <- c("Label","Text")

length(which(!complete.cases(spam.raw)))

# It is useful to convert class labels to factors early on
spam.raw$Label <- factor(spam.raw$Label)


# Prop table express them as fractions
prop.table(table(spam.raw$Label))
table(spam.raw$Label)

# Let's look at the relative lengths of texts

spam.raw$TextLength <- nchar(spam.raw$Text)
summary(spam.raw$TextLength)

# This turns out to be a great feature for classification
library(ggplot2)
ggplot(spam.raw,aes(x = TextLength, fill = Label))+
    geom_histogram(binwidth = 5)+
    theme_bw()

# Data split (stratifed split: ie: class proportions are maintained during the split)
library(caret)

set.seed(32984)
indexes <- createDataPartition(y = spam.raw$Label,
                               p = 0.7, list = FALSE)

train <- spam.raw[indexes,]
test <- spam.raw[-indexes,]

# Note that class label proportions are maintained
prop.table(table(train$Label))
prop.table(table(test$Label))
```

# More data exploration and processing

How do we represent text as Data Frame? We achieve this by **TOKENIZATION**.

Once Tokenization is completed we can create a **Document-Frequency Matrix** (DFM).

- Each row represents a distinct **document**.
- Each column represents a distinct **token**.(Distinct Tokens across all documents are also called **Terms**)
- Each cell(matrix entry) contains the **counts** of that token for a given document.

- one-grams produce **bag-of-words model**, this is where we point we start, it is possible to preserve word order by adding n-grams to make our models even stronger or accurate.

## Some considerations

- Do we want all tokens to be terms in our DFM?
    - How about case-sensitivity/capitalization
    - Punctuation? Do I want them in my DFM? Typically you don't.
    - Do you want numbers/digits in your DFM?
    - Do you want evey word? No: don't use stop words (e.g: the,and..)
    - Symbols (sometimes very important)
    - What about similar words? **STEMMING** Is it possible to COLLAPSE similar word to a common stem (single representation).


# Data Pipelines

Notice we will have some dirty text, such as **&amp** here. Our pipelines should take these into consideration, possibly adding domain knowledge into it. Replacing, stripping/removing these are all important decisions.
    
```{r}
train$Text[21]
```

```{r}
train$Text[38]
```

**It is reccomended to streamline the steps below as a Pipeline:**

### Tokenization

```{r}
# Quite powerful package
library(quanteda)

# Tokenize
train.tokens <- tokens(train$Text,what = "word",
                       remove_numbers = TRUE, 
                       remove_punct = TRUE,
                       remove_symbols = TRUE, 
                       remove_hyphens = TRUE)

# Returns a list-like object that contains tokens
train.tokens[[357]]

```

Notice that based on our preferences the tokenization is performed.

If we want 3-grams, simply use ngrams argument:

(Note that the default concetanator is "_")

```{r}
tokens(train$Text,what = "word",
                       remove_numbers = TRUE, 
                       remove_punct = TRUE,
                       remove_symbols = TRUE, 
                       remove_hyphens = TRUE,
       ngrams = 3)[[357]]
```

### Transform Tokens

Let's convert the tokens to lowercase (for our use-case)

```{r}
train.tokens <- tokens_tolower(train.tokens)
train.tokens[[357]]
```

### Removing stopwords

This is a tricky step for any text analytics pipeline. We need to understand what is in the stop words library of each package we might be using. Depending on our domain, the list might contain words that we may actually want to maintain in our DFM.

These are the stopwords removed by the quanteda package:

```{r}
quanteda::stopwords()
```

```{r}
# Remove stopwords
train.tokens <- tokens_select(x = train.tokens,
                              pattern = stopwords(),
                              selection = "remove")
train.tokens[[357]]
```

### Stemming tokens

```{r}
train.tokens <- tokens_wordstem(train.tokens, 
                                language = "english")
train.tokens[[357]]
```

What we have gone through is almost a typical text preprocessing pipeline:

1. Tokenize (specify what to remove; numbers, symbols, hyphens...)
2. Lovercase
3. Remove stopwords (or custom common words?)
4. Stemming

# Create Document Frequency Matrix

In our case this is a bag-of-words model since we used one-grams;

```{r}
# Use quantida function dfm
train.tokens.dfm <- dfm(x = train.tokens,
                        tolower = FALSE)

# Generates a fairly large matrix:
dim(train.tokens.dfm)

# Convert to standard matrix:
train.tokens.matrix <- as.matrix(train.tokens.dfm)

head(train.tokens.matrix[,1:30])
```

 Note that our feature space/dimentionality increased dramatically.
 
 2 facts to notice:
 
 1. Text Analytics suffers from **curse of dimensionality**.
 2. Text Analytics creates a matrix with mostly zeros (**sparsity problem**), which we will try to deal with using **feature extraction**.
 
```{r}
# Investigating the effects of stemming
colnames(train.tokens.matrix)[1:50]
```
 
This pretty much completes the standard text data processing pipeline.

# Building our First Model

We will build our model using cross-validation. 

DFM is contains our corpus (corpus is a fancy name for a collection of documents) and terms(features).

We set up a feature data frame with labels:
```{r}
# Collecting everything in a standard dataframe:
train.tokens.df <- cbind(Label= train$Label,
                         convert(train.tokens.dfm, to = "data.frame"))
head(train.tokens.df[,1:10])
```

### Fix the names of data frame:

Note that terms we generated by tokenization requires some additional processing:

```{r}
names(train.tokens.df)[c(146,148,235,238)]
```

Note that these are not **valid column names** for data frames in R. Machine learning algorithms will throw error unless we transform them using **makes.names()** function:

```{r}
names(train.tokens.df) <- make.names(names(train.tokens.df))
names(train.tokens.df)[c(146,148,235,238)]
```


## Setting up cross-validation

We need to perform **stratified cross-validation** given the class-imbalance in our data set. In other words, in our CV folds, we need to make sure the representation of the individual classes are similar to the entire training data.

```{r}
library(caret)
set.seed(48743)
cv.folds <- createMultiFolds(y = train$Label,
                             k = 10,
                             times = 3)

# 3 times means we will create 30 random stratified samples

cv.cntrl <- trainControl(method = "repeatedcv",
                         number = 10,
                         repeats = 3,
                         index = cv.folds)
 
# Note that setting index = cv.folds makes us ensure that
# trainControl will use the stratified folds we specified above
# If we don't specifiy this, the model will still train but
# it will use random folds, which may not be the desired stratified
# folds we wish to obtain due to class imbalance
```

The 10 fold cross-calidation repeated 3 times gives a more robust estimate of performance (but will be computationally expensive). Particularly when there is class-imbalance like in this example, **repeatedcv** might be useful.

Note that the size of our data-frame is not trivial, hence we will perform parallel multi-core computing using doSNOW package:

```{r}
# Check the number of cores
parallel:::detectCores()
```

doSNOW works for both Mac and Windows:

```{r,eval=FALSE}
library(doSNOW)
start.time <- Sys.time()

# Create a cluster to work on 7 logical cores (keep at least 1 core for the operating system)
# type = "SOCK" Socket cluster
# Behind the scenes it will create 7 jobs
cl <- makeCluster(spec = 7,type = "SOCK")
# Register the cluster: 
registerDoSNOW(cl)
# Once registered, caret will recognize this cluster and parallelize the job

rpart.cv.1 <- caret::train(Label ~ ., data = train.tokens.df,
                           method = 'rpart',trControl = cv.cntrl,
                           tuneLength = 7)

# Processing (training) is done, so we stop the cluster:
stopCluster(cl)

end.time <- Sys.time()
```

We are using **rpart**, single decision tree for an initial model to develop an intuition. We use ALL the features in the DFM. 

Note that:

    **tuneLength = 7** performs effectively hyperparameter tuning using 7 different model fits comparing 7 different hyperparameter values for rpart tree.
    
```{r,eval = FALSE}
end.time - start.time

# Time difference of 8.363868 mins
```
Save the model object and reload for future use:

```{r}
#saveRDS(rpart.cv.1,"rpart_cv_1.rds")
rpart.cv.1 <- readRDS("rpart_cv_1.rds")
rpart.cv.1
```

Note the results are presented quite intuitively,**cp** is the rpart hyperparameter being tuned and 7 distinct values of that parameter was presented. The best cp value is choosen based on accuracy. 94% accuracy is actually quite good out of the box. Next we will try to improve this benchmark by:

- using TF-IDF (by transforming the nature of the data we have)
- adding n-grams into our set of features (hoping to get more useful features)
- extract features using SVD, to reduce the dimensionality
- and finally trying more powerful algorithms like randomForest

# Using TF-IDF: changing representation of DFM

So far we realized that the bag-of-words model with document frequency matrices could work! 94% Accuracy is quite remarkable in simplest terms.

There is still room for improvement:

- Longer documents will tend to have higher term counts.
- Terms that appear frequently across the corpus aren't as important (nearly zero variance).

Therefore, we can improve upon the DFM representation if we can achieve the following:

- Normalize documents based on their length
- Penalize terms that occur frequently across the corpus (set of available documents)

So, if we can adjust our DFM to accomodate these 2 things, it will be much more powerful.

This is exactly what TF-IDF address:

## TF: Term Frequency 

- Let freq(t,d) be the count of the instances of the term t in document d.

- Let TF(t,d) be the proportion of the count of term t in document d. e.g: if I have a term that appeared in a document 4 times and the document has a total of 10 words (after text processing pipeline), then TF of that term becomes 0.4.

Mathematically:

    TF(t,d) = freq(t,d) / Sum i (freq(ti,d))

Therefore, TF achieves the first goal, that is the normalization to text length.

## IDF: Inverse Document Frequency

- Let N be the count of distinct documents in the corpus.
- Let count(t) be the count of documents in the corpus in which the term t is present.

Then,

    IDF(t) = log( N / count(t) )
    
    # log10 is commonly used.
    
Notice that if a term appears in ALL of the documents, IDF of that term will be log 1 = 0, which will be a penalizing weight for that term. Hence, if the term appears in every single document, this would mean the information in that term is not useful, because it does not explain any variability. This way we achieved our second goal, that is penalizing the terms that occur frequently across the corpus.

## TF-IDF idea:

IF we combine TF and IDF, we can enhance the document-term frequency matrices. TF-IDF is simply multiplication of these two amounts:

    TF-IDF(t,d) = TF(t,d) * IDF(t)
    
In most cases TF-IDF is prototypical for text processing pipelines as it can enhance the features of the DFM. Hence, along with other steps described above, TF-IDF is often incorporated in to text-processing pipeline. 

Let's run the TF-IDF using our DFM:

Note that we are writing our own functions for this because:

1. its educational.
2. we can cache the IDF from the training data and we can use the same weights to transform the test data set to get the consistent representation.

```{r}
# Our Function for calculating Term Frequency(TF)

term.frequency <- function(row){
    return(row / sum(row))
}

# Our function for calculating Inverse Document Frequency (IDF)
inverse.doc.freq <- function(col){
    corpus.size <- length(col)
    doc.count <- length(which(col > 0))
    
    return(log10(corpus.size/doc.count))
}

# Our function for calculating TF-IDF
tf.idf <- function(tf,idf){
    return (tf * idf)
}

# First step, normalize all documents via TF
# Matrix transformations:

train.tokens.df <- apply(train.tokens.matrix,1,term.frequency)
dim(train.tokens.df)


```
```{r}
dim(train.tokens.matrix)
```

Note that the matrix got transposed as a result of the tf transformation:

```{r}
head(train.tokens.df[,1:10],20)
```

Notice that now the documents are columns and terms are rows.

The second step, calculate the **IDF vector** that we will use for transforming both training and test data. This is very important, because we will train the model using these set of IDFs, then when we want to transform the test data we should be able to transform the data to exactly the same space.

```{r}
# Apply the idf function over the columns
train.tokens.idf <- apply(train.tokens.matrix,2,inverse.doc.freq)
str(train.tokens.idf)
```

Note that the idf is a single numeric vector of idfs as we expected.

Finally, calculate the TF-IDF for our training corpus:
```{r}
# Note that since train.tokens.df was transposed during the normalization, we apply the tf.idf function on columns
train.tokens.tfidf <- apply(train.tokens.df,2,tf.idf, 
                            idf = train.tokens.idf)
dim(train.tokens.tfidf)
```

I still maintain the transposed matrix state, but the data is now multiplied by the IDF weights for each term:

```{r}
head(train.tokens.tfidf[,1:10],20)
```

    
After this transformations, we achieved our goals. Now the values also reflect the impact of how often a particular term is being seen in the document. As a result, if the TF-IDF value is low, that means the term is relatively frequent across the corpus and that is reflected (e.g: value of the word **go** is quite low, which is intuitive because it would be a common word. In contrast, the word **jurong** has a higher value in text1, which could imply its possibly higher predictive value relative to word **go**).

Importantly, we need to **transpose this matrix back** into the original form of the matrix, where columns are features and rows are documents.

```{r}
# Transpose the matrix to original shape
train.tokens.tfidf <- t(train.tokens.tfidf)
dim(train.tokens.tfidf)
head(train.tokens.tfidf[,1:10],20)
```


Check for incomplete cases:

```{r}
length(which(!complete.cases(train.tokens.tfidf)))
```

Note that as a result of the pre-processing pipeline, it is possible to end up with empty strings in our matrix. Imagine that some words could be just punctuations or similar characters that were stripped, hence missing values in this matrix do occur, because any time there is an error in the calculation of TF-IDF there would be NAN errors.

Fix the incomplete cases:

```{r}
incomplete.cases <- which(!complete.cases(train.tokens.tfidf))
train$Text[incomplete.cases]
```
Note that we expect that all these cases will be stripped off by our pre-processing pipeline. We need to correct these documents.

```{r}
# We fill these documents with zero values, instead of removing them. This is because these messages could be legitimate messages, but our processing pipeline can not extract any features from them.
train.tokens.tfidf[incomplete.cases,] <- rep(0,0,ncol(train.tokens.tfidf))
dim(train.tokens.tfidf)
sum(which(!complete.cases(train.tokens.tfidf)))
```

We have now fixed these incomplete cases, so that machine learning algorithms will not throw error.

Lastly, lets combine this matrix with labels and make the column names legitimate as we have done for the DFM previously:

```{r}
train.tokens.tfidf.df <- cbind(Label = train$Label,
                              data.frame(train.tokens.tfidf))

names(train.tokens.tfidf.df) <- make.names(names(train.tokens.tfidf.df))

head(train.tokens.tfidf.df[,1:10],10)
```

## Refitting the rpart single decision tree

Let's see if using TF-IDF improved the performance of the same model we fitted previously by using the raw DFM:

```{r,eval=FALSE}
library(doSNOW)
start.time <- Sys.time()

# Create a cluster to work on 7 logical cores (keep at least 1 core for the operating system)
# type = "SOCK" Socket cluster
# Behind the scenes it will create 7 jobs
cl <- makeCluster(spec = 7,type = "SOCK")
# Register the cluster: 
registerDoSNOW(cl)
# Once registered, caret will recognize this cluster and parallelize the job

rpart.cv.2 <- caret::train(Label ~ ., data = train.tokens.tfidf.df,
                           method = 'rpart',trControl = cv.cntrl,
                           tuneLength = 7)

# Processing (training) is done, so we stop the cluster:
stopCluster(cl)

end.time <- Sys.time()

end.time - start.time
```


```{r}
#saveRDS(rpart.cv.2,"rpart_cv_2.rds")
rpart.cv.2 <- readRDS("rpart_cv_2.rds")
rpart.cv.2
```


Good! So note that the accuracy is uplifted from 0.942 to 0.945, hence TF-IDF transformation helped to improve model's predictive performance.
    
# N-grams

Now we have TF-IDF transformed Document Term Matrix. Can we further improve upon this representation? N-grams can be one way:

- Our representations so far have been single terms (unigrams)
- We can have more complex N-grams, this will help to capture some signal from word ordering.

Hence, we can add n-grams during the text processing pipeline.

**But be careful, by adding even bigrams, we will significantly increase the size of the matrix! This will ever increase the SPARSITY and CURSE OF DIMENSIONALITY problems.**

Adding bigrams to our feature matrix:
```{r}
# Notice we can reuse our existing token object:
train.tokens <- tokens_ngrams(train.tokens, n = 1:2)
train.tokens[[357]]
```

Now, we need to run the entire text processing pipeline on this new matrix:

(Note that stopword removal,lowercasing and stemming has been already performed since we used the original train.tokens object.)

```{r}
# Use quantida function dfm
train.tokens.dfm <- dfm(x = train.tokens,
                        tolower = FALSE)

# Convert to standard matrix:
train.tokens.matrix <- as.matrix(train.tokens.dfm)

# Apply the idf function over the columns (need to cache this for test set)
train.tokens.idf <- apply(train.tokens.matrix,2,inverse.doc.freq)

# Need to increase memory limit otherwise won't fit into memory
memory.limit(size=56000)

# Note that since train.tokens.df was transposed during the normalization, we apply the tf.idf function on columns
train.tokens.tfidf <- apply(train.tokens.df,2,tf.idf, 
                            idf = train.tokens.idf)

# Transpose the matrix to original shape
train.tokens.tfidf <- t(train.tokens.tfidf)


# Fix incomplete cases
incomplete.cases <- which(!complete.cases(train.tokens.tfidf))
train.tokens.tfidf[incomplete.cases,] <- rep(0,0,ncol(train.tokens.tfidf))


# Combine with label and make legitimate names
train.tokens.tfidf.df <- cbind(Label = train$Label,
                              data.frame(train.tokens.tfidf))

names(train.tokens.tfidf.df) <- make.names(names(train.tokens.tfidf.df))

```

Note the nice feature of quantida DFMs, you can type the name of the object and get information:

```{r}
train.tokens.dfm
```

Note how the dimensions are significantly expanded just by adding bigrams, and the 99.9% of the matrix is sparse!


Important: we should clean up unsued memory:

```{r}
gc()
```

This function is sometimes useful to call to clean up unused memory. However, in text analytics we can easily reach the memory boundaries and it may or may not help.


# LSA: Latent Semantic Analysis using SVD

We will perform feature extraction!

Two purposes:

1. We want to make our columns more feature rich than the current highly sparse state. We want to shrink the dimension, yet maintaining the variance.
2. By reducing the size of the data, we want to be able to use more powerful (but also more complex and computationally costly) algorithms for our problem (otherwise we will be running big scalability problems).

## Vector space model

**Vector space model** helps to address our current problems related to curse of dimensionality and scaleability.

- Core intuition: we represent documents as **vectors of numbers**.
- Our representation allows us to work with document geometrically.

The idea is explained well here (statring from 12:12):

```{r}
library(vembedr)
vembedr::embed_url("https://www.youtube.com/watch?v=Fza5szojsU8&index=7&list=PL8eNk_zTBST8olxIRFoo0YeXxEOkYdoxi")
```


### Dot product of documents

When we start thinking documents as mathematical vectors, we can intuitively start seeing some documents are more alike. 

We can even use trigonometrical distances to understand document relationships!

Dot product of vectors A and B (vectors have to be in the same length):

A.B = Sum (Ai * Bi)

Dot product of two document vectors can be used as a proxy for correlation. Hence, conceptually dot products will be useful.

We can leverage matrix multiplication to calculate all dot products of all vectors all in once:

    X: document term matrix

    Dot product of all Docs = X * X Transpose
    
    
**Intuition:** the dot product of the documents is indicative of document correlation given the set of matrix terms.

### Dot product of the terms

We can also take the perspective of taking the dot products of the terms in the document-term frequency matrix!
    
    X: document term matrix
    
    Dot Product of All Terms = X Transpose * X

In this case we are getting a proxy for correlation between the terms!

**Intuition:** the dot product of the terms is indicative of term correlation given the set of matrix documents.

## Latent Semantic Analysis

**Intuition:** Extract relationships between the documents and terms assuming that terms that are close in meaning will appear in similar (i.e: correlated) pieces of text.

**Implementation**: LSA leverages a singular value decomposition (SVD) factorization of a term-document matrix to extract these relationships.

SVD: is a way of decomposing a matrix into smaller, more compressed forms.

    X: term-document matrix (transposed version of document-term matrix)

    SVD of X = X = U * Sigma * V Transpose

where:

    U: matrix contains the eigenvectors of the term correlations, X * X Transpose
    V: matrix contains the eigenvectors of the document correlations, X Transpose * X
    Sigma: contains the singular values of the factorization
    
this could be simplified as:

    U: contains the correlations between terms, AT higher levels of abstraction
    V: contains the correlations between documents, AT higher levels of abstraction
    
Here, the abstraction helps to compress the signal from various columns in the original matrix into a single column.
    
LSA often remediates the curse of dimensionality problem in text analytics:

- The matrix factorization has the effect of combining columns, potentially enriching signal in the data.

- By selecting a fraction of the most important singular values, LSA can dramatically reduce dimensionality.

However, there is not free lunch:

- Performing the SVD factorization is computationally intensive.
- The reduced factorized matrices (i.e: the "semantic spaces") are approximations. So, they won't be 'exact', there will be some information loss.But hopefully the overall gain of predictive power will be higher than some information loss by compressing the data.
- We will need to project new data into the semantic space.

SVD is effective and is almost a staple of text analytics pipelines.

## Projecting New Data

- As with TF-IDF, the use of SVD will require that new data be transformed/projected before predictions can be made.

- Once the basic processing (tokenization, removal of stop words, stemming...),the following represents the high-level process for projection:

    - Normalize the document vector (i.e:rows) using the term.frequency() function
    - Complete the TF-IDF projection by using the tf.idf() function
    - Apply the SVD projection on the document vector

Mathematically, the SVD projection for document d is:

    - d_hat = sigma.inverse * U.transpose %*% d
    
```{r,eval= FALSE}
# We use this package to perform SVD matrix factorization, because this performs TRUNCATED SVD (X number of most significant features), hence it will allow the algorithm run faster and enables to scale

library(irlba)
start.time <- Sys.time()

# Note that LSA assumes TDM, so we have to transpose our DTM

# Perform SVD specifically, reduce the dimensionality down to 300 columns
train.irlba <- irlba(t(train.tokens.tfidf), nv = 300, maxit= 600)

Sys.time() - start.time
# Time difference of 6.302957 mins
```
```{r}
#saveRDS(train.irlba,"train_irlba.rds")
train.irlba <- readRDS("train_irlba.rds")
```

## Cached SVD elements from training data

As with TF-IDF we will need to project the new data during production into the SVD
semantic space. We will use the mathematical representation we discussed above to achieve this. We will also need to cache the train.irlba object (as we saved as rds file) since it contains the SVD factorized matrices of the training data set and it tehrefore contains the mathematical elements to project data into the new latent semantic space:

    - d_hat = sigma.inverse * U.transpose %*% d

```{r}
sigma.inverse <- 1 / train.irlba$d
u.transpose <- t(train.irlba$u)

# For example, let's say we have a new document (a new TF-IDFed data point) and we want to project it into the same LSA space:
document <- train.tokens.tfidf[1,]

# document_hat becomes the transformed version of this new data point
document_hat <- sigma.inverse * u.transpose %*% document 

```

Notice that document_hat now has 300 columns:

```{r}
str(document)
str(document_hat)
```

This way we achieve to shrink and project any new data point with the same exact tokens into the same latent semantic space.    

# Cosine Similarity

We can engineer yet another feature to assess the similarity between the documents in the large vector space.A better proxy than the dot product.

Measure the **angle** between the document vectors, and take their **cosine**.Then, comparing cosines of the angles between different documents will give us a better idea on which documents are more likely to be similar to each other than other documents.  

- Why exactly using the cosine between document vectors is an improvement over the dot product? 

Some advantages of using cosine for document similarity:

1. Given our representations, the cosine will be between [0,1]
2. Metric works well in high dimensional spaces (of course, you better to SVD to shrink the dimensions, before measuring the cosine of the angles).


The idea is explained well here (statring from 9:22):

```{r}
library(vembedr)
vembedr::embed_url("https://www.youtube.com/watch?v=7cwBhWYHgsA&list=PL8eNk_zTBST8olxIRFoo0YeXxEOkYdoxi&index=10")
```

Note that how pythagorian theorem scales to work well on higher dimensions as well.

Note: **Do not interpret cos values as percentages!! eg: if cosine is 0.73, we do NOT say these two documents are 73% similar! This is not to interpret it!!**

How to interpret cos similarity:

e.g:

1. cos between doc1 and doc2: 0.73
2. cos between doc1 and doc3: 0.95

We interpret that **doc1 and doc3** are more similar to each other than **doc1 and doc2.** We can also say that the second similarity is about 20% higher than the first similarity. 

- dlsa library in R can effectively calculate cos similarity matrix for a given DFM or SVD transformed version of it.

New feature engineering idea: One way to use cosine similarity in a classification problem is to use **average similarity** between that particular data point and all other data points of the same class.If you calculate this for all data points in the training data, this will become a new feature and sometimes it is a powerfull feature (e.g: spam similarity)

    
# Your First Test

In order to test the performance of the models we generated, we need to pre-process the test data set in the same way we performed for the training set. This is essential for the utility of models during production, since as the new data flows it becomes necessary to convert the new data into the new format and process exactly in the same way model can understand:

Recall what we have performed with training data, we need to perform the same steps with the test data:

1. Tokenization
2. Lower casing
3. Stopword removal
4. Stemming
5. Adding bigrams
6. Transform to dfm
7. **Ensure test dfm has the same features as train dfm**

```{r}
# 1.Tokenization
test.tokens <- tokens(test$Text, what = "word",
                      remove_numbers = TRUE, remove_punct = TRUE,
                      remove_symbols = TRUE, remove_hyphens = TRUE)

# 2.Lowercase the tokens
test.tokens <- tokens_tolower(test.tokens)

# 3.Stopword removal
test.tokens <- tokens_select(test.tokens, stopwords(),
                             selection = "remove")

# 4. Stemming
test.tokens <- tokens_wordstem(test.tokens, language = "english")

# 5. Add bi-grams
test.tokens <- tokens_ngrams(test.tokens, n = 1:2)

# 6. Convert n-grams to quanteda document-term frequency matrix
test.tokens.dfm <- dfm(test.tokens,tolower = FALSE)
```

Now compare dimensions of training and test DFMs:

```{r}
train.tokens.dfm
test.tokens.dfm
```

Note that test data set will extract different features, this is totally normal and is a common theme in text analytics. However, our model will only understand the same 29,154 features that it was trained to work for. Therefore, we need to ensure the processed test data set is structured in the same way.

Watch the relevant discussion here (more interesting from 6:48):

```{r}
library(vembedr)
vembedr::embed_url("https://www.youtube.com/watch?v=XWUi7RivDJY&index=11&list=PL8eNk_zTBST8olxIRFoo0YeXxEOkYdoxi")
```

Now, we need to make sure test DFM looks exactly the same as training interms of columns (features):

```{r}
# 7. **Ensure test dfm has the same features as train dfm**
test.tokens.dfm <- dfm_select(test.tokens.dfm,train.tokens.dfm)
test.tokens.dfm
```

Note that using the train.tokens.dfm as the second argument (pattern), we impose the occurance of the same tokens in the test data set.

Next step in our pipeline is to TF-IDF new data using the values we cached previously:

```{r}
# TF
test.tokens.matrix <- as.matrix(test.tokens.dfm)
## Normalize all documents using TF
test.tokens.df <- apply(test.tokens.matrix,1,term.frequency)
str(test.tokens.df)
```
Recall that now documents are on columns and terms are in rows (i.e: TDM)

```{r}
## IDF
# Note we are using the idf vector generated by the training set to project the test data into the same space:
test.tokens.tfidf <- apply(test.tokens.df,2,tf.idf,idf = train.tokens.idf)
dim(test.tokens.tfidf)
```

Note that it is still TDM.

Final steps:

1. Transpose back to DFM
2. Fix incomplete cases

```{r}
# Transpose the matrix back to DFM
test.tokens.tfidf <- t(test.tokens.tfidf)
dim(test.tokens.tfidf)
```
```{r}
# Fix incomplete cases
summary(test.tokens.tfidf[1,])
test.tokens.tfidf[is.na(test.tokens.tfidf)] <- 0.0
summary(test.tokens.tfidf[1,])
```

3. SVD projection:

We use the TF-IDF projected data to project it into the training LSA semantic space using the cached vector and matrices we produced from the SVD decompusition of the training data:

```{r}
# test.svd.raw <- t(sigma.inverse * g.transpose %*% t(test.tokens.tfidf))

# This should give back 300 columns (due to the selected semantic space training data resides in)

```

