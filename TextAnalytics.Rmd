---
title: "Text Analytics with R"
output:
  pdf_document:
    toc: yes
  word_document:
    toc: yes
    highlight: default
    df_print: kable
  html_document:
    depth: 6
    df_print: paged
    highlight: espresso
    number_sections: yes
    theme: cerulean
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(results = "markup", fig.align = "center",
                      fig.height= 8, fig.width= 8,message=FALSE,warning=FALSE)
```

# Introduction

Here is initial data processing and manipulation.

```{r}
############################################
# Text Analytics (learning from D.Langer)
# Mainly following playlist at : https://www.youtube.com/watch?v=4vuw0AsHeGw&list=PL8eNk_zTBST8olxIRFoo0YeXxEOkYdoxi
############################################

#install.packages(c("quanteda","irlba"))

spam.raw <- read.csv(file = 'spam.csv' , stringsAsFactors =  FALSE)
spam.raw <- spam.raw[,1:2]

names(spam.raw) <- c("Label","Text")

length(which(!complete.cases(spam.raw)))

# It is useful to convert class labels to factors early on
spam.raw$Label <- factor(spam.raw$Label)


# Prop table express them as fractions
prop.table(table(spam.raw$Label))
table(spam.raw$Label)

# Let's look at the relative lengths of texts

spam.raw$TextLength <- nchar(spam.raw$Text)
summary(spam.raw$TextLength)

# This turns out to be a great feature for classification
library(ggplot2)
ggplot(spam.raw,aes(x = TextLength, fill = Label))+
    geom_histogram(binwidth = 5)+
    theme_bw()

# Data split (stratifed split: ie: class proportions are maintained during the split)
library(caret)

set.seed(32984)
indexes <- createDataPartition(y = spam.raw$Label,
                               p = 0.7, list = FALSE)

train <- spam.raw[indexes,]
test <- spam.raw[-indexes,]

# Note that class label proportions are maintained
prop.table(table(train$Label))
prop.table(table(test$Label))
```

# More data exploration and processing

How do we represent text as Data Frame? We achieve this by **TOKENIZATION**.

Once Tokenization is completed we can create a **Document-Frequency Matrix** (DFM).

- Each row represents a distinct **document**.
- Each column represents a distinct **token**.(Distinct Tokens across all documents are also called **Terms**)
- Each cell(matrix entry) contains the **counts** of that token for a given document.

- one-grams produce **bag-of-words model**, this is where we point we start, it is possible to preserve word order by adding n-grams to make our models even stronger or accurate.

## Some considerations

- Do we want all tokens to be terms in our DFM?
    - How about case-sensitivity/capitalization
    - Punctuation? Do I want them in my DFM? Typically you don't.
    - Do you want numbers/digits in your DFM?
    - Do you want evey word? No: don't use stop words (e.g: the,and..)
    - Symbols (sometimes very important)
    - What about similar words? **STEMMING** Is it possible to COLLAPSE similar word to a common stem (single representation).


# Data Pipelines

Notice we will have some dirty text, such as **&amp** here. Our pipelines should take these into consideration, possibly adding domain knowledge into it. Replacing, stripping/removing these are all important decisions.
    
```{r}
train$Text[21]
```

```{r}
train$Text[38]
```

**It is reccomended to streamline the steps below as a Pipeline:**

### Tokenization

```{r}
# Quite powerful package
library(quanteda)

# Tokenize
train.tokens <- tokens(train$Text,what = "word",
                       remove_numbers = TRUE, 
                       remove_punct = TRUE,
                       remove_symbols = TRUE, 
                       remove_hyphens = TRUE)

# Returns a list-like object that contains tokens
train.tokens[[357]]

```

Notice that based on our preferences the tokenization is performed.

If we want 3-grams, simply use ngrams argument:

(Note that the default concetanator is "_")

```{r}
tokens(train$Text,what = "word",
                       remove_numbers = TRUE, 
                       remove_punct = TRUE,
                       remove_symbols = TRUE, 
                       remove_hyphens = TRUE,
       ngrams = 3)[[357]]
```

### Transform Tokens

Let's convert the tokens to lowercase (for our use-case)

```{r}
train.tokens <- tokens_tolower(train.tokens)
train.tokens[[357]]
```

### Removing stopwords

This is a tricky step for any text analytics pipeline. We need to understand what is in the stop words library of each package we might be using. Depending on our domain, the list might contain words that we may actually want to maintain in our DFM.

These are the stopwords removed by the quanteda package:

```{r}
quanteda::stopwords()
```

```{r}
# Remove stopwords
train.tokens <- tokens_select(x = train.tokens,
                              pattern = stopwords(),
                              selection = "remove")
train.tokens[[357]]
```

### Stemming tokens

```{r}
train.tokens <- tokens_wordstem(train.tokens, 
                                language = "english")
train.tokens[[357]]
```

What we have gone through is almost a typical text preprocessing pipeline:

1. Tokenize (specify what to remove; numbers, symbols, hyphens...)
2. Lovercase
3. Remove stopwords (or custom common words?)
4. Stemming

# Create Document Frequency Matrix

In our case this is a bag-of-words model since we used one-grams;

```{r}
# Use quantida function dfm
train.tokens.dfm <- dfm(x = train.tokens,
                        tolower = FALSE)

# Generates a fairly large matrix:
dim(train.tokens.dfm)

# Convert to standard matrix:
train.tokens.matrix <- as.matrix(train.tokens.dfm)

head(train.tokens.matrix[,1:30])
```

 Note that our feature space/dimentionality increased dramatically.
 
 2 facts to notice:
 
 1. Text Analytics suffers from **curse of dimensionality**.
 2. Text Analytics creates a matrix with mostly zeros (**sparsity problem**), which we will try to deal with using **feature extraction**.
 
```{r}
# Investigating the effects of stemming
colnames(train.tokens.matrix)[1:50]
```
 
This pretty much completes the standard text data processing pipeline.

# Building our First Model

We will build our model using cross-validation. 

DFM is contains our corpus (corpus is a fancy name for a collection of documents) and terms(features).

We set up a feature data frame with labels:
```{r}
# Collecting everything in a standard dataframe:
train.tokens.df <- cbind(Label= train$Label,
                         convert(train.tokens.dfm, to = "data.frame"))
head(train.tokens.df[,1:10])
```

### Fix the names of data frame:

Note that terms we generated by tokenization requires some additional processing:

```{r}
names(train.tokens.df)[c(146,148,235,238)]
```

Note that these are not **valid column names** for data frames in R. Machine learning algorithms will throw error unless we transform them using **makes.names()** function:

```{r}
names(train.tokens.df) <- make.names(names(train.tokens.df))
names(train.tokens.df)[c(146,148,235,238)]
```


## Setting up cross-validation

We need to perform **stratified cross-validation** given the class-imbalance in our data set. In other words, in our CV folds, we need to make sure the representation of the individual classes are similar to the entire training data.

```{r}
library(caret)
set.seed(48743)
cv.folds <- createMultiFolds(y = train$Label,
                             k = 10,
                             times = 3)

# 3 times means we will create 30 random stratified samples

cv.cntrl <- trainControl(method = "repeatedcv",
                         number = 10,
                         repeats = 3,
                         index = cv.folds)
 
# Note that setting index = cv.folds makes us ensure that
# trainControl will use the stratified folds we specified above
# If we don't specifiy this, the model will still train but
# it will use random folds, which may not be the desired stratified
# folds we wish to obtain due to class imbalance
```

The 10 fold cross-calidation repeated 3 times gives a more robust estimate of performance (but will be computationally expensive). Particularly when there is class-imbalance like in this example, **repeatedcv** might be useful.

Note that the size of our data-frame is not trivial, hence we will perform parallel multi-core computing using doSNOW package:

```{r}
# Check the number of cores
parallel:::detectCores()
```

doSNOW works for both Mac and Windows:

```{r,eval=FALSE}
library(doSNOW)
start.time <- Sys.time()

# Create a cluster to work on 7 logical cores (keep at least 1 core for the operating system)
# type = "SOCK" Socket cluster
# Behind the scenes it will create 7 jobs
cl <- makeCluster(spec = 7,type = "SOCK")
# Register the cluster: 
registerDoSNOW(cl)
# Once registered, caret will recognize this cluster and parallelize the job

rpart.cv.1 <- caret::train(Label ~ ., data = train.tokens.df,
                           method = 'rpart',trControl = cv.cntrl,
                           tuneLength = 7)

# Processing (training) is done, so we stop the cluster:
stopCluster(cl)

end.time <- Sys.time()
```

We are using **rpart**, single decision tree for an initial model to develop an intuition. We use ALL the features in the DFM. 

Note that:

    **tuneLength = 7** performs effectively hyperparameter tuning using 7 different model fits comparing 7 different hyperparameter values for rpart tree.
    
```{r,eval = False}
end.time - start.time

# Time difference of 8.363868 mins
```
Save the model object and reload for future use:

```{r}
#saveRDS(rpart.cv.1,"rpart_cv_1.rds")
rpart.cv.1 <- readRDS("rpart_cv_1.rds")
rpart.cv.1
```

Note the results are presented quite intuitively,**cp** is the rpart hyperparameter being tuned and 7 distinct values of that parameter was presented. The best cp value is choosen based on accuracy. 94% accuracy is actually quite good out of the box. Next we will try to improve this benchmark by:

- using TF-IDF (by transforming the nature of the data we have)
- adding n-grams into our set of features (hoping to get more useful features)
- extract features using SVD, to reduce the dimensionality
- and finally trying more powerful algorithms like randomForest

# Using TF-IDF: changing representation of DFM

So far we realized that the bag-of-words model with document frequency matrices could work! 94% Accuracy is quite remarkable in simplest terms.

There is still room for improvement:

- Longer documents will tend to have higher term counts.
- Terms that appear frequently across the corpus aren't as important (nearly zero variance).

Therefore, we can improve upon the DFM representation if we can achieve the following:

- Normalize documents based on their length
- Penalize terms that occur frequently across the corpus (set of available documents)

So, if we can adjust our DFM to accomodate these 2 things, it will be much more powerful.

This is exactly what TF-IDF address:

## TF: Term Frequency 

- Let freq(t,d) be the count of the instances of the term t in document d.

- Let TF(t,d) be the proportion of the count of term t in document d. e.g: if I have a term that appeared in a document 4 times and the document has a total of 10 words (after text processing pipeline), then TF of that term becomes 0.4.

Mathematically:

    TF(t,d) = freq(t,d) / Sum i (freq(ti,d))

Therefore, TF achieves the first goal, that is the normalization to text length.

## IDF: Inverse Document Frequency

- Let N be the count of distinct documents in the corpus.
- Let count(t) be the count of documents in the corpus in which the term t is present.

Then,

    IDF(t) = log( N / count(t) )
    
    # log10 is commonly used.
    
Notice that if a term appears in ALL of the documents, IDF of that term will be log 1 = 0, which will be a penalizing weight for that term. Hence, if the term appears in every single document, this would mean the information in that term is not useful, because it does not explain any variability. This way we achieved our second goal, that is penalizing the terms that occur frequently across the corpus.

## TF-IDF idea:

IF we combine TF and IDF, we can enhance the document-term frequency matrices. TF-IDF is simply multiplication of these two amounts:

    TF-IDF(t,d) = TF(t,d) * IDF(t)
    
In most cases TF-IDF is prototypical for text processing pipelines as it can enhance the features of the DFM. Hence, along with other steps described above, TF-IDF is often incorporated in to text-processing pipeline. 

Let's run the TF-IDF using our DFM:

Note that we are writing our own functions for this because:

1. its educational.
2. we can cache the IDF from the training data and we can use the same weights to transform the test data set to get the consistent representation.

```{r}
# Our Function for calculating Term Frequency(TF)

term.frequency <- function(row){
    return(row / sum(row))
}

# Our function for calculating Inverse Document Frequency (IDF)
inverse.doc.freq <- function(col){
    corpus.size <- length(col)
    doc.count <- length(which(col > 0))
    
    return(log10(corpus.size/doc.count))
}

# Our function for calculating TF-IDF
tf.idf <- function(tf,idf){
    return (tf * idf)
}

# First step, normalize all documents via TF
# Matrix transformations:

train.tokens.df <- apply(train.tokens.matrix,1,term.frequency)
dim(train.tokens.df)


```
```{r}
dim(train.tokens.matrix)
```

Note that the matrix got transposed as a result of the tf transformation:

```{r}
head(train.tokens.df[,1:10],20)
```

Notice that now the documents are columns and terms are rows.

The second step, calculate the **IDF vector** that we will use for transforming both training and test data. This is very important, because we will train the model using these set of IDFs, then when we want to transform the test data we should be able to transform the data to exactly the same space.

```{r}
# Apply the idf function over the columns
train.tokens.idf <- apply(train.tokens.matrix,2,inverse.doc.freq)
str(train.tokens.idf)
```

Note that the idf is a single numeric vector of idfs as we expected.

Finally, calculate the TF-IDF for our training corpus:
```{r}
# Note that since train.tokens.df was transposed during the normalization, we apply the tf.idf function on columns
train.tokens.tfidf <- apply(train.tokens.df,2,tf.idf, 
                            idf = train.tokens.idf)
dim(train.tokens.tfidf)
```

I still maintain the transposed matrix state, but the data is now multiplied by the IDF weights for each term:

```{r}
head(train.tokens.tfidf[,1:10],20)
```

    
After this transformations, we achieved our goals. Now the values also reflect the impact of how often a particular term is being seen in the document. As a result, if the TF-IDF value is low, that means the term is relatively frequent across the corpus and that is reflected (e.g: value of the word **go** is quite low, which is intuitive because it would be a common word. In contrast, the word **jurong** has a higher value in text1, which could imply its possibly higher predictive value relative to word **go**).

Importantly, we need to **transpose this matrix back** into the original form of the matrix, where columns are features and rows are documents.

```{r}
train.tokens.tfidf <- t(train.tokens.tfidf)
dim(train.tokens.tfidf)
head(train.tokens.tfidf[,1:10],20)
```


Check for incomplete cases:

```{r}
length(which(!complete.cases(train.tokens.tfidf)))
```

Note that as a result of the pre-processing pipeline, it is possible to end up with empty strings in our matrix. Imagine that some words could be just punctuations or similar characters that were stripped, hence missing values in this matrix do occur, because any time there is an error in the calculation of TF-IDF there would be NAN errors.

Fix the incomplete cases:

```{r}
incomplete.cases <- which(!complete.cases(train.tokens.tfidf))
train$Text[incomplete.cases]
```
Note that we expect that all these cases will be stripped off by our pre-processing pipeline. We need to correct these documents.

```{r}
# We fill these documents with zero values, instead of removing them. This is because these messages could be legitimate messages, but our processing pipeline can not extract any features from them.
train.tokens.tfidf[incomplete.cases,] <- rep(0,0,ncol(train.tokens.tfidf))
dim(train.tokens.tfidf)
sum(which(!complete.cases(train.tokens.tfidf)))
```

We have now fixed these incomplete cases, so that machine learning algorithms will not throw error.

Lastly, lets combine this matrix with labels and make the column names legitimate as we have done for the DFM previously:

```{r}
train.tokens.tfidf.df <- cbind(Label = train$Label,
                              data.frame(train.tokens.tfidf))

names(train.tokens.tfidf.df) <- make.names(names(train.tokens.tfidf.df))

head(train.tokens.tfidf.df[,1:10],10)
```

    



